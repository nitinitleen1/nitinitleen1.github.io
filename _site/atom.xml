<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-05-28T22:48:35+05:30</updated><id>http://localhost:4000/atom.xml</id><title type="html">Nitin Kumar Singh</title><subtitle>My Learning and Adventures in Computer Science and Life</subtitle><author><name>Nitin</name></author><entry><title type="html">The Two Sum Problem - LeetCode</title><link href="http://localhost:4000/leetcode-two-sum/" rel="alternate" type="text/html" title="The Two Sum Problem - LeetCode" /><published>2023-05-28T00:00:00+05:30</published><updated>2023-05-28T00:00:00+05:30</updated><id>http://localhost:4000/leetcode-two-sum</id><content type="html" xml:base="http://localhost:4000/leetcode-two-sum/">&lt;h2 id=&quot;the-two-sum-problem&quot;&gt;The Two Sum Problem&lt;/h2&gt;

&lt;p&gt;The Two Sum problem is a classic coding question often asked in programming interviews. The problem statement is as follows:&lt;/p&gt;

&lt;p&gt;Given an array of integers &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nums&lt;/code&gt; and an integer target value &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target&lt;/code&gt;, find two numbers in the array that add up to the target and return their indices.&lt;/p&gt;

&lt;p&gt;Here’s an example to illustrate the problem:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input: nums = [2, 7, 11, 15], target = 9
Output: [0, 1]
Explanation: The numbers at indices 0 and 1 (2 and 7) add up to 9.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;approach-and-solution&quot;&gt;Approach and Solution&lt;/h2&gt;

&lt;p&gt;To solve the Two Sum problem, we can use a simple approach that utilizes a hashmap to store the complement of each number as we iterate through the array.&lt;/p&gt;

&lt;p&gt;Here’s the step-by-step solution in Python:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;twoSum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Create an empty hashmap
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;hashmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Iterate through the array
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Calculate the complement
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;complement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Check if the complement is already in the hashmap
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;complement&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hashmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Return the indices of the two numbers
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hashmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Add the current number and its index to the hashmap
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;hashmap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# If no solution is found, return an empty list
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;We initialize an empty dictionary &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_dict&lt;/code&gt; to store numbers and their indices.&lt;/li&gt;
  &lt;li&gt;We iterate through the array using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enumerate()&lt;/code&gt; function to access both the indices and the corresponding numbers.&lt;/li&gt;
  &lt;li&gt;For each number, we calculate its complement by subtracting it from the target.&lt;/li&gt;
  &lt;li&gt;We check if the complement exists in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_dict&lt;/code&gt;. If it does, we have found the two numbers that add up to the target, and we return their indices.&lt;/li&gt;
  &lt;li&gt;If the complement is not in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_dict&lt;/code&gt;, we add the current number and its index to the dictionary for future reference.&lt;/li&gt;
  &lt;li&gt;If we exhaust the array without finding a solution, we return an empty list.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s test the solution with a sample input:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twoSum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;div class=&quot;language-csharp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;complexity-analysis&quot;&gt;Complexity Analysis&lt;/h2&gt;

&lt;p&gt;The above solution has a time complexity of O(n) since we iterate through the array once. The space complexity is also O(n) since we use a hashmap to store the numbers and their indices.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Through this blog post, we explored the popular LeetCode problem “Two Sum” and provided a detailed solution using a hash table. The key takeaway from this problem is to leverage data structures effectively to optimize our algorithms. LeetCode offers a myriad of challenges like this one, allowing programmers to enhance their problem-solving skills and gain confidence in tackling coding interviews. Remember, practice makes perfect!&lt;/p&gt;</content><author><name>Nitin</name></author><category term="Programming" /><category term="leetcode" /><category term="python" /><category term="algorithms" /><summary type="html">The Two Sum Problem</summary></entry><entry><title type="html">Setting up an Airflow Cluster</title><link href="http://localhost:4000/scaling-out-airflow/" rel="alternate" type="text/html" title="Setting up an Airflow Cluster" /><published>2020-01-07T21:13:00+05:30</published><updated>2020-01-07T21:13:00+05:30</updated><id>http://localhost:4000/scaling-out-airflow</id><content type="html" xml:base="http://localhost:4000/scaling-out-airflow/">&lt;p&gt;Data-driven companies often hinge their business intelligence and product development on the execution of complex data pipelines. These pipelines are often referred to as data workflows, a term that can be somewhat opaque in that workflows are not limited to one specific definition and do not perform a specific set of functions per se. To orchestrate these workflows there are lot of schedulers like oozie, Luigi, Azkaban and Airflow. This blog demonstrate the setup of one of these orchestrator i.e Airflow.&lt;/p&gt;

&lt;h2 id=&quot;a-shot-intro&quot;&gt;A shot intro:&lt;/h2&gt;

&lt;p&gt;There are many orchestrators which are there in the technology space but Airflow provides a slight edge if our requirement hinges on of the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No cron – With Airflows included scheduler we don’t need to rely on cron to schedule our DAG and only use one framework (not like Luigi)&lt;/li&gt;
  &lt;li&gt;Code Bases – In Airflow all the workflows, dependencies, and scheduling are done in python code. Therefore, it is rather easy to build complex structures and extend the flows.&lt;/li&gt;
  &lt;li&gt;Language – Python is a language somewhat natural to pick up and was available on our team.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However setting up a production grade setup required some effort and this blog address the same.&lt;/p&gt;

&lt;h2 id=&quot;basic-tech-terms&quot;&gt;Basic Tech Terms:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Metastore:&lt;/strong&gt; Its a database which stores information regarding the state of tasks. Database updates are performed using an abstraction layer implemented in SQLAlchemy. This abstraction layer cleanly separates the function of the remaining components of Airflow from the database.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Executor&lt;/strong&gt;: The Executor is a message queuing process that is tightly bound to the Scheduler and determines the worker processes that actually execute each scheduled task.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scheduler:&lt;/strong&gt; The Scheduler is a process that uses DAG definitions in conjunction with the state of tasks in the metadata database to decide which tasks need to be executed, as well as their execution priority. The Scheduler is generally run as a service.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Worker:&lt;/strong&gt; These are the processes that actually execute the logic of tasks, and are determined by the Executor being used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-architecture&quot;&gt;AWS Architecture:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/airflow-schematic.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Airflow provides an option to utilize CeleryExecutor to execute tasks in distributed fashion. In this mode, we can run several servers each running multiple worker nodes to execute the tasks. This mode uses Celery along with a message queueing service RabbitMQ.The diagram show the interactivity between different component services i.e. Airflow(Webserver and Scheduler), Celery(Executor) and RabbitMQ and Metastore in an AWS Environment.&lt;/p&gt;

&lt;p&gt;For simplicity of the blog, we will demonstrate the setup of a single node master server and a single node worker server. Below is the following details for the setup:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;EC2 Master Node - Running Scheduler and Webserver&lt;/li&gt;
  &lt;li&gt;EC2 Worker Node - Running Celery Executor and Workers&lt;/li&gt;
  &lt;li&gt;RDS Metastore - Storing information about metadata and dag&lt;/li&gt;
  &lt;li&gt;EC2 Rabbit MQ Nodes - Running RabbitMq broker&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;enviornment-prerequisite&quot;&gt;Enviornment Prerequisite:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Operating System: Ubuntu 16.04/Ubuntu 18.04 / Debian System&lt;/li&gt;
  &lt;li&gt;Python Environment: Python 3.5x&lt;/li&gt;
  &lt;li&gt;DataBase: PostgreSql v11.2 (RDS)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once the prerequisites are taken care of, we can proceed with the installation.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation:&lt;/h2&gt;

&lt;p&gt;The first step of the setup is the to configure the RDS Postgres database for airflow.
For that we need to connect to RDS Database using using admin user.For the sake of simplicity we are using command line utility from one of the EC2 servers to connect to our RDS Server. For command line client installation for postgres database on debian system execute following commands to install and execute.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Client Installation
apt-get &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; update 
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;postgresql-client&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the client is installed try to connect to the Database using the admin user.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Generating IAM Token
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RDSHOST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{host_name}&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PGPASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;aws rds generate-db-auth-token &lt;span class=&quot;nt&quot;&gt;--hostname&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$RDSHOST&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt; 5432 &lt;span class=&quot;nt&quot;&gt;--region&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;region&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;admin_user&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Connecting to the Database
psql &lt;span class=&quot;s2&quot;&gt;&quot;host=hostName port=portNumber dbname=DBName user=userName -password&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;sslmode and sslrootcert parameter is used when we are using SSL/TLS based connection. For more information refere &lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.AWSCLI.PostgreSQL.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once the connection is established with the database create a database named airflow which will act as a primary source where all the metadata,scheduler and other information will be stored by airflow.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DATABASE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;USER&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATABASE_USER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;WITH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PASSWORD&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;‘{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATABASE_USER_PASSWORD&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}’&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GRANT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ALL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;PRIVILEGES&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DATABASE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TO&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATABASE_USER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GRANT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CONNECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;DATABASE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;TO&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATABASE_USER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the above step is done the next step is to setup rabbitMQ in one the EC2 server. To install it follow the steps defined below.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Login as root&lt;/li&gt;
  &lt;li&gt;Install RabbitMQ Server&lt;/li&gt;
  &lt;li&gt;Verify status&lt;/li&gt;
  &lt;li&gt;Install RabbitMQ Web Interface&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;rabbitmq-server
rabbitmqctl status
rabbitmq-plugins &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;rabbitmq_management&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Enable and start rabbitMQ server and add users and permissions to it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;c1&quot;&gt;-- Enable rabbitMQ server&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rabbitmq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;enable&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rabbitmq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;start&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rabbitmq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;-- Add Users and permissions&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rabbitmqctl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add_user&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RABBITMQ_USER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RABBITMQ_USER_PASSWORD&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rabbitmqctl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set_user_tags&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RABBITMQ_USER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;administrator&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Make Virtual Host and Set Permission for the host.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;rabbitmqctl add_vhost &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;VIRTUALHOST_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
rabbitmqctl set_permissions &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;VIRTUALHOST_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;RABBITMQ_USER&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; “.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;” “.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;” “.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;”&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Download the rabbitadmin utility&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;wget http://127.0.0.1:15672/cli/rabbitmqadmin
&lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x rabbitmqadmin&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;One Final step is to make a queue.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;./rabbitmqadmin &lt;span class=&quot;nb&quot;&gt;declare &lt;/span&gt;queue –username&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;RABBITMQ_USER&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; –password&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;RABBITMQ_USER_PASSWORD&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; –vhost&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;VIRTUALHOST_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;QUEUE_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;durable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can now access the RabbitMQ UI utility by hitting the public IP at port 15672.&lt;/p&gt;

&lt;p&gt;If you someone want to setup multiple machines to work as a RabbitMQ Cluster you can refere &lt;a href=&quot;https://www.rabbitmq.com/clustering.html&quot;&gt;here&lt;/a&gt;.
Now we have all the building blocks. The final step is to setup airflow using celery.&lt;/p&gt;

&lt;h2 id=&quot;setting-up-of-airflow-using-celery--&quot;&gt;Setting up of Airflow Using Celery :-&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Install required libraries and dependencies for airflow on each node i.e worker and master.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;apt-get update
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;build-essential
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;python3-pip
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;python-dev python3-dev libsasl2-dev gcc
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libffi-dev
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;libkrb5-dev
apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;python-pandas

&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; To get the pip version 
which pip&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Install airflow and celery on each of the machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;pyamqp
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;psycopg2
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;apache-airflow[postgres,rabbitmq,celery]
airflow version


&lt;span class=&quot;nt&quot;&gt;--Celery&lt;/span&gt; Installation 
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;celery&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;4.3.0


&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Initializing airflow
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;AIRFLOW_HOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/airflow &lt;span class=&quot;c&quot;&gt;#(provide any directory for airflow home)&lt;/span&gt;
airflow initdb&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;configuration&quot;&gt;Configuration:&lt;/h2&gt;

&lt;p&gt;We now have airflow installed on all the nodes we have to change but to detect external database and broker we need to do conf changes hence to do that, Edit the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/airflow/aiflow.cfg&lt;/code&gt; on all the nodes&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; On Nodes change following parameters

&lt;span class=&quot;nv&quot;&gt;executor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; CeleryExecutor
&lt;span class=&quot;nv&quot;&gt;sql_alchemy_conn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; postgresql+psycopg2://airflow:airflow@&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;HOSTNAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/airflow 
&lt;span class=&quot;nv&quot;&gt;broker_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; pyamqp://guest:guest@&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;RabbitMQ-HOSTNAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:5672/
&lt;span class=&quot;nv&quot;&gt;celery_result_backend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; db+postgresql://airflow:airflow@&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;HOSTNAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/airflow 
&lt;span class=&quot;nv&quot;&gt;dags_are_paused_at_creation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; True
&lt;span class=&quot;nv&quot;&gt;load_examples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; False
&lt;span class=&quot;nv&quot;&gt;default_queue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;QUEUE_NAME&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the changes are done run reload apache to detect those change on each node:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;airflow initdb&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Start webserver and schduler on Master Node and Worker on Second i.e Worker Node and we are done!!!&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Node 1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Master Node&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
airflow webserver &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8000
airflow scheduler

&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Node 2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Worker Node&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
airflow worker&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The only things is left is to synchronize the dags across the machines i.e master and worker.
There are multiple ways to synchronise but for simplicity I created a cron in each machine which will sync my git repo from the remote git repositories to the folder my dags code were located.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Remove any unneccessary files from dag folder
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/airflow/dags
&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Clone dags from git repo
&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;git
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/airflow/dags
git clone URL &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; Creating a cron &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;pulling every 5 mins 
crontab &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;/5 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;aboslute_path&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/airflow/dags &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; git pull&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;However this is not a production grade implementation as I would recommed either a common mountpoint like EFS or any General NFS Servers or using a configuration Management tool like ansible, chef or puppet.&lt;/p&gt;

&lt;h2 id=&quot;more-tuning&quot;&gt;More Tuning:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Parallelism&lt;/strong&gt; : This parameter determines the maximum number of task instances that can be actively running in parallel across the entire airflow deployment. For example, if it is set to 10 there can’t be more than 10 tasks running irrespective of the number of dags. Hence, this is the maximum number of active tasks at any time. Set it depending upon no of machines in a cluster i.e for one Node 16 is recommended.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dag Concurrency&lt;/strong&gt; : This parameter determines the number of task instances that can be scheduled per DAG.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;worker_concurrency&lt;/strong&gt; : This parameter determines the number of tasks each worker node can run at any given time. For example, if it is set to 10 then the worker node can concurrently execute 10 tasks that have been scheduled by the schedule.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://site.clairvoyantsoft.com/making-apache-airflow-highly-available/&quot;&gt;Making Apache Airflow Highly Available&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.qubole.com/en/latest/user-guide/data-engineering/airflow/config-airflow-cluster.html&quot;&gt;Best Practices for Setup Airflow &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@khatri_chetan/challenges-and-struggle-while-setting-up-multi-node-airflow-cluster-7f19e998ebb&quot;&gt;Common mistakes and Challenges while running Multinode Airflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://airflow.apache.org/docs/stable/installation.html&quot;&gt;Airflow documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.Connecting.AWSCLI.PostgreSQL.html&quot;&gt;How to Connect to RDS Postgres&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Nitin</name></author><category term="Airflow" /><category term="airflow" /><category term="postgresql" /><category term="data-pipeline" /><category term="aws" /><category term="rabbitmq" /><summary type="html">Data-driven companies often hinge their business intelligence and product development on the execution of complex data pipelines. These pipelines are often referred to as data workflows, a term that can be somewhat opaque in that workflows are not limited to one specific definition and do not perform a specific set of functions per se. To orchestrate these workflows there are lot of schedulers like oozie, Luigi, Azkaban and Airflow. This blog demonstrate the setup of one of these orchestrator i.e Airflow.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/airflow-image1.jpg" /><media:content medium="image" url="http://localhost:4000/assets/airflow-image1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>